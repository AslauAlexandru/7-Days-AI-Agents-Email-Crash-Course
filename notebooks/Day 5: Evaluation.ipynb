{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0fc80cc",
   "metadata": {},
   "source": [
    "# Day 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83bb224b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# print(os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21943985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "\n",
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f212a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='groq:llama-3.1-8b-instant'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8494069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84961e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e747ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4897406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, the search results were not very helpful. However, based on general knowledge, I can provide some steps to get you started with using Docker on Windows.\n",
      "\n",
      "To use Docker on Windows, you will need to:\n",
      "\n",
      "1. **Install Docker Desktop**: Download and install Docker Desktop from the official Docker website. This will give you the Docker CLI, a GUI dashboard, and a few other tools.\n",
      "2. **Create a Docker account**: If you don't already have a Docker account, create one on the Docker website. This will give you access to the Docker Hub, where you can pull and push images.\n",
      "3. **Pull a Docker image**: Use the `docker pull` command to download a Docker image from Docker Hub. For example, to pull the official Ubuntu image, run `docker pull ubuntu`.\n",
      "4. **Run a Docker container**: Use the `docker run` command to start a new container from the image you pulled. For example, to run a new Ubuntu container, run `docker run -it ubuntu`.\n",
      "5. **Use the Docker CLI**: The Docker CLI is used to manage containers, images, volumes, and more. You can use the `docker` command followed by various subcommands to perform tasks.\n",
      "\n",
      "Please note that these are general steps and may not cover all the features and nuances of using Docker on Windows.\n",
      "\n",
      "If you encounter any specific issues or have questions about the process, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_20250930_022336_b7fae0.json')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Try these questions:\n",
    "\n",
    "how do I use docker on windows?\n",
    "can I join late and get a certificate?\n",
    "what do I need to do for the certificate?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac56d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='groq:llama-3.1-8b-instant'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "999b13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9e809b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f63e9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='groq:llama-3.1-8b-instant',\n",
    "    output_type=EvaluationChecklist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "116ef1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b22611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f11a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
    "log_record = load_log_file('/workspaces/7-Days-AI-Agents-Email-Crash-Course/logs/faq_agent_20250930_022336_b7fae0.json')\n",
    "\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a73144a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelHTTPError",
     "evalue": "status_code: 400, model_name: llama-3.1-8b-instant, body: {'error': {'message': \"tool call validation failed: attempted to call tool 'brave_search' which was not in request.tools\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=brave_search>{\"query\": \"using docker on windows\"} </function>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/models/groq.py:280\u001b[39m, in \u001b[36mGroqModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    279\u001b[39m     extra_headers.setdefault(\u001b[33m'\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m'\u001b[39m, get_user_agent())\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m    281\u001b[39m         model=\u001b[38;5;28mself\u001b[39m._model_name,\n\u001b[32m    282\u001b[39m         messages=groq_messages,\n\u001b[32m    283\u001b[39m         n=\u001b[32m1\u001b[39m,\n\u001b[32m    284\u001b[39m         parallel_tool_calls=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    285\u001b[39m         tools=tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    286\u001b[39m         tool_choice=tool_choice \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    287\u001b[39m         stop=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    288\u001b[39m         stream=stream,\n\u001b[32m    289\u001b[39m         response_format=response_format \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    290\u001b[39m         max_tokens=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    291\u001b[39m         temperature=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    292\u001b[39m         top_p=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    293\u001b[39m         timeout=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    294\u001b[39m         seed=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    295\u001b[39m         presence_penalty=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    296\u001b[39m         reasoning_format=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mgroq_reasoning_format\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    297\u001b[39m         frequency_penalty=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    298\u001b[39m         logit_bias=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    299\u001b[39m         extra_headers=extra_headers,\n\u001b[32m    300\u001b[39m         extra_body=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mextra_body\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    301\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/resources/chat/completions.py:930\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[33;03mCreates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    777\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    928\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    931\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/openai/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    932\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    933\u001b[39m         {\n\u001b[32m    934\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    935\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    936\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcompound_custom\u001b[39m\u001b[33m\"\u001b[39m: compound_custom,\n\u001b[32m    937\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdisable_tool_validation\u001b[39m\u001b[33m\"\u001b[39m: disable_tool_validation,\n\u001b[32m    938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: documents,\n\u001b[32m    939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexclude_domains\u001b[39m\u001b[33m\"\u001b[39m: exclude_domains,\n\u001b[32m    940\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    941\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    942\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    943\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minclude_domains\u001b[39m\u001b[33m\"\u001b[39m: include_domains,\n\u001b[32m    944\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minclude_reasoning\u001b[39m\u001b[33m\"\u001b[39m: include_reasoning,\n\u001b[32m    945\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    946\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    947\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    948\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    949\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    950\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    951\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    952\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    953\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m    954\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_format\u001b[39m\u001b[33m\"\u001b[39m: reasoning_format,\n\u001b[32m    955\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m    956\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msearch_settings\u001b[39m\u001b[33m\"\u001b[39m: search_settings,\n\u001b[32m    957\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    958\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    959\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    960\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m    961\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    962\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    963\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    964\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    965\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    966\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    967\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    968\u001b[39m         },\n\u001b[32m    969\u001b[39m         completion_create_params.CompletionCreateParams,\n\u001b[32m    970\u001b[39m     ),\n\u001b[32m    971\u001b[39m     options=make_request_options(\n\u001b[32m    972\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    973\u001b[39m     ),\n\u001b[32m    974\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m    975\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    976\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m    977\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/_base_client.py:1762\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1759\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1760\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1761\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/_base_client.py:1576\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1575\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1578\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"tool call validation failed: attempted to call tool 'brave_search' which was not in request.tools\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=brave_search>{\"query\": \"using docker on windows\"} </function>'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModelHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n\u001b[32m      3\u001b[39m checklist = result.output\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(checklist.summary)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/agent/abstract.py:218\u001b[39m, in \u001b[36mAbstractAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, deferred_tool_results, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, event_stream_handler)\u001b[39m\n\u001b[32m    204\u001b[39m event_stream_handler = event_stream_handler \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_stream_handler\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    207\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    208\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m     toolsets=toolsets,\n\u001b[32m    217\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    220\u001b[39m             \u001b[38;5;28mself\u001b[39m.is_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_call_tools_node(node)\n\u001b[32m    221\u001b[39m         ):\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m node.stream(agent_run.ctx) \u001b[38;5;28;01mas\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/run.py:149\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    147\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n\u001b[32m    148\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(node=next_node):\n\u001b[32m    151\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_graph/graph.py:758\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_graph/graph.py:731\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    729\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    730\u001b[39m         ctx = GraphRunContext(state=\u001b[38;5;28mself\u001b[39m.state, deps=\u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    734\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:399\u001b[39m, in \u001b[36mModelRequestNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._did_stream:\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# `self._result` gets set when exiting the `stream` contextmanager, so hitting this\u001b[39;00m\n\u001b[32m    396\u001b[39m     \u001b[38;5;66;03m# means that the stream was started but not finished before `run()` was called\u001b[39;00m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.AgentRunError(\u001b[33m'\u001b[39m\u001b[33mYou must finish streaming before calling run()\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_request(ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:441\u001b[39m, in \u001b[36mModelRequestNode._make_request\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    440\u001b[39m model_settings, model_request_parameters, message_history, _ = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prepare_request(ctx)\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m model_response = \u001b[38;5;28;01mawait\u001b[39;00m ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n\u001b[32m    442\u001b[39m ctx.state.usage.requests += \u001b[32m1\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish_handling(ctx, model_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/models/groq.py:186\u001b[39m, in \u001b[36mGroqModel.request\u001b[39m\u001b[34m(self, messages, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    184\u001b[39m check_allow_model_requests()\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._completions_create(\n\u001b[32m    187\u001b[39m         messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(GroqModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[32m    188\u001b[39m     )\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ModelHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.body, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# pragma: no branch\u001b[39;00m\n\u001b[32m    191\u001b[39m         \u001b[38;5;66;03m# The Groq SDK tries to be helpful by raising an exception when generated tool arguments don't match the schema,\u001b[39;00m\n\u001b[32m    192\u001b[39m         \u001b[38;5;66;03m# but we'd rather handle it ourselves so we can tell the model to retry the tool call.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/models/groq.py:304\u001b[39m, in \u001b[36mGroqModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (status_code := e.status_code) >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ModelHTTPError(status_code=status_code, model_name=\u001b[38;5;28mself\u001b[39m.model_name, body=e.body) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mModelHTTPError\u001b[39m: status_code: 400, model_name: llama-3.1-8b-instant, body: {'error': {'message': \"tool call validation failed: attempted to call tool 'brave_search' which was not in request.tools\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=brave_search>{\"query\": \"using docker on windows\"} </function>'}}"
     ]
    }
   ],
   "source": [
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ff0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output \n",
    "\n",
    "\n",
    "#log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
    "log_record = load_log_file('/workspaces/7-Days-AI-Agents-Email-Crash-Course/logs/faq_agent_20250930_022336_b7fae0.json')\n",
    "\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc590c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='groq:llama-3.1-8b-instant',\n",
    "    output_type=QuestionsList\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f77efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0a55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What operating system is ideal for the data engineering course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:06,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No specific information was found regarding the ideal operating system for the data engineering course. However, most data engineering tasks can be performed on any operating system that supports the necessary tools and software, such as Linux, macOS, or Windows. It's generally recommended to use a 64-bit operating system with at least 8 GB of RAM and a multi-core processor for optimal performance. \n",
      "\n",
      "For more information, you can refer to the course materials or contact the course instructor.\n",
      "\n",
      "How do I add Anaconda to my system's PATH?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that the search results did not provide a clear answer to your question. However, I can offer some general guidance.\n",
      "\n",
      "To add Anaconda to your system's PATH, you typically need to follow these steps:\n",
      "\n",
      "1. **Find the Anaconda installation directory**: This is usually located in your user directory, such as `~/anaconda3` or `~/opt/anaconda3`.\n",
      "2. **Locate the Anaconda bin directory**: Within the Anaconda installation directory, there should be a `bin` directory that contains the executable files for Anaconda.\n",
      "3. **Add the bin directory to your system's PATH environment variable**: The exact steps to do this vary depending on your operating system. On Linux or macOS, you can usually add a line to your shell configuration file (e.g., `~/.bashrc` or `~/.zshrc`) that exports the PATH variable with the Anaconda bin directory included. On Windows, you can right-click on \"Computer\" or \"This PC\", select \"Properties\", then click on \"Advanced system settings\" and finally click on \"Environment Variables\".\n",
      "\n",
      "For more detailed instructions, I recommend checking the [official Anaconda documentation](https://github.com/DataTalksClub/faq/blob/main/README.md) or searching for specific instructions for your operating system.\n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "How can I find the network name in Docker?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:06,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that the search results did not provide the information you are looking for. \n",
      "\n",
      "To find the network name in Docker, you can use the command `docker network ls` in your terminal. This will list all the networks available in your Docker environment, along with their names and other details.\n",
      "\n",
      "If you are looking for a specific network, you can use the `docker network inspect` command followed by the network ID or name to get more detailed information about that network.\n",
      "\n",
      "For more information, you can refer to the [Docker documentation](https://docs.docker.com/engine/reference/commandline/network_ls/) on Docker networking. \n",
      "\n",
      "Please note that the information provided is general guidance and may not be specific to your course materials. If you have any further questions or need more specific information, feel free to ask.\n",
      "\n",
      "How do I set the environment variable for DLT_DATA_DIR?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:03<00:05,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, the search results did not provide a specific answer to your question. However, I can offer some general guidance.\n",
      "\n",
      "The environment variable DLT_DATA_DIR is likely used to specify the directory where data for the course is stored. To set an environment variable, you can typically use the export command in your terminal, followed by the variable name and the desired value. For example:\n",
      "\n",
      "export DLT_DATA_DIR=/path/to/data/directory\n",
      "\n",
      "You can replace /path/to/data/directory with the actual path where you want to store the data.\n",
      "\n",
      "If you are using a Jupyter notebook or another environment, the process for setting environment variables may be different. You may need to consult the documentation for your specific environment or contact the course instructors for further guidance.\n",
      "\n",
      "For more information, you can refer to the course materials, such as the [README](https://github.com/DataTalksClub/faq/blob/main/README.md) or other relevant files in the course repository.\n",
      "\n",
      "What is the solution to the UnicodeDecodeError when reading data into a pandas dataframe?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:03<00:03,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function=text_search{\"query\": \"UnicodeDecodeError pandas dataframe solution\"}</function>\n",
      "\n",
      "Why is 'PULocationID' not recognized in PostgreSQL?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:04<00:02,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function=text_search{\"query\": \"PULocationID PostgreSQL not recognized\"}</function>\n",
      "\n",
      "How can I use pgcli within a Docker container?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:04<00:01,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function=text_search {\"query\": \"pgcli Docker container\"} </function>\n",
      "\n",
      "How do I convert the difference between two TimestampType values to hours in PySpark?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:04<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function=text_search {\"query\": \"PySpark TimestampType to hours\"} </function>\n",
      "\n",
      "How do I integrate a DLT pipeline into Apache Airflow?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:05<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, the search did not return any relevant results. \n",
      "\n",
      "To integrate a DLT (Data Loading and Transformation) pipeline into Apache Airflow, you would typically use Airflow's built-in operators and hooks to interact with your DLT system.\n",
      "\n",
      "Here are some general steps you can follow:\n",
      "1. Define your DLT pipeline: Determine the data sources, data transformations, and data loading processes that make up your DLT pipeline.\n",
      "2. Create Airflow DAG: Create a new DAG in Airflow that outlines the tasks and dependencies involved in your DLT pipeline.\n",
      "3. Use Airflow operators: Use Airflow's built-in operators (such as BashOperator, PythonOperator, etc.) to execute the tasks in your DLT pipeline. You can also create custom operators if needed.\n",
      "4. Configure hooks: Configure Airflow hooks to interact with your DLT system. For example, you might use a hook to connect to a database or a cloud storage service.\n",
      "5. Test and deploy: Test your DAG to ensure it runs successfully, then deploy it to your Airflow production environment.\n",
      "\n",
      "For more specific guidance, I recommend checking the official [Apache Airflow documentation](https://airflow.apache.org/docs/) or seeking out tutorials and examples specific to your use case.\n",
      "\n",
      "What are the steps to resolve the issue of pip not working with Anaconda?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that the search results did not provide any relevant information to answer your question. \n",
      "\n",
      "However, I can provide some general guidance on how to resolve the issue of pip not working with Anaconda. \n",
      "\n",
      "1. Check if you have the latest version of Anaconda and pip installed. You can do this by running `conda update --all` and `pip install --upgrade pip` in your terminal.\n",
      "2. Make sure that you are using the correct version of pip. Anaconda comes with its own version of pip, which can be different from the system's default pip. You can check which version of pip you are using by running `which pip` in your terminal.\n",
      "3. Try resetting the package cache by running `conda clean --all` in your terminal.\n",
      "4. If none of the above steps work, you can try reinstalling Anaconda or seeking help from the Anaconda community or support team.\n",
      "\n",
      "For more information, you can refer to the [official Anaconda documentation](https://github.com/DataTalksClub/faq/blob/main/README.md) or [troubleshooting guide](https://github.com/DataTalksClub/faq/blob/main/README.md).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6563ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:10,  1.22s/it]\n"
     ]
    },
    {
     "ename": "ModelHTTPError",
     "evalue": "status_code: 400, model_name: llama-3.3-70b-versatile, body: {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=final_result><![CDATA[{\\n\"checklist\": [\\n  {\\n    \"check_name\": \"Ideal Operating System\",\\n    \"check_pass\": false,\\n    \"justification\": \"No specific information was found regarding the ideal operating system for the data engineering course. However, most data engineering tasks can be performed on any operating system that supports the necessary tools and software, such as Linux, macOS, or Windows.\"\\n  },\\n  {\\n    \"check_name\": \"System Recommendations\",\\n    \"check_pass\": true,\\n    \"justification\": \"It\\'s generally recommended to use a 64-bit operating system with at least 8 GB of RAM and a multi-core processor for optimal performance.\"\\n  }\\n],\\n\"summary\": \"No specific operating system is ideal for the data engineering course, but a 64-bit operating system with at least 8 GB of RAM and a multi-core processor is recommended for optimal performance. For more information, refer to the course materials or contact the course instructor. [Course Materials](https://github.com/DataTalksClub/faq/blob/main/README.md)\"\\n}]></function>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/models/groq.py:280\u001b[39m, in \u001b[36mGroqModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    279\u001b[39m     extra_headers.setdefault(\u001b[33m'\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m'\u001b[39m, get_user_agent())\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m    281\u001b[39m         model=\u001b[38;5;28mself\u001b[39m._model_name,\n\u001b[32m    282\u001b[39m         messages=groq_messages,\n\u001b[32m    283\u001b[39m         n=\u001b[32m1\u001b[39m,\n\u001b[32m    284\u001b[39m         parallel_tool_calls=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    285\u001b[39m         tools=tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    286\u001b[39m         tool_choice=tool_choice \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    287\u001b[39m         stop=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    288\u001b[39m         stream=stream,\n\u001b[32m    289\u001b[39m         response_format=response_format \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    290\u001b[39m         max_tokens=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    291\u001b[39m         temperature=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    292\u001b[39m         top_p=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    293\u001b[39m         timeout=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    294\u001b[39m         seed=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    295\u001b[39m         presence_penalty=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    296\u001b[39m         reasoning_format=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mgroq_reasoning_format\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    297\u001b[39m         frequency_penalty=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    298\u001b[39m         logit_bias=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    299\u001b[39m         extra_headers=extra_headers,\n\u001b[32m    300\u001b[39m         extra_body=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mextra_body\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    301\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/resources/chat/completions.py:930\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[33;03mCreates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    777\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    928\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    931\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/openai/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    932\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    933\u001b[39m         {\n\u001b[32m    934\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    935\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    936\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcompound_custom\u001b[39m\u001b[33m\"\u001b[39m: compound_custom,\n\u001b[32m    937\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdisable_tool_validation\u001b[39m\u001b[33m\"\u001b[39m: disable_tool_validation,\n\u001b[32m    938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: documents,\n\u001b[32m    939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexclude_domains\u001b[39m\u001b[33m\"\u001b[39m: exclude_domains,\n\u001b[32m    940\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    941\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    942\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    943\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minclude_domains\u001b[39m\u001b[33m\"\u001b[39m: include_domains,\n\u001b[32m    944\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minclude_reasoning\u001b[39m\u001b[33m\"\u001b[39m: include_reasoning,\n\u001b[32m    945\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    946\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    947\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    948\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    949\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    950\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    951\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    952\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    953\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m    954\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_format\u001b[39m\u001b[33m\"\u001b[39m: reasoning_format,\n\u001b[32m    955\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m    956\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msearch_settings\u001b[39m\u001b[33m\"\u001b[39m: search_settings,\n\u001b[32m    957\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    958\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    959\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    960\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m    961\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    962\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    963\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    964\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    965\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    966\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    967\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    968\u001b[39m         },\n\u001b[32m    969\u001b[39m         completion_create_params.CompletionCreateParams,\n\u001b[32m    970\u001b[39m     ),\n\u001b[32m    971\u001b[39m     options=make_request_options(\n\u001b[32m    972\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    973\u001b[39m     ),\n\u001b[32m    974\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m    975\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    976\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m    977\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/_base_client.py:1762\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1759\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1760\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1761\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/_base_client.py:1576\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1575\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1578\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=final_result><![CDATA[{\\n\"checklist\": [\\n  {\\n    \"check_name\": \"Ideal Operating System\",\\n    \"check_pass\": false,\\n    \"justification\": \"No specific information was found regarding the ideal operating system for the data engineering course. However, most data engineering tasks can be performed on any operating system that supports the necessary tools and software, such as Linux, macOS, or Windows.\"\\n  },\\n  {\\n    \"check_name\": \"System Recommendations\",\\n    \"check_pass\": true,\\n    \"justification\": \"It\\'s generally recommended to use a 64-bit operating system with at least 8 GB of RAM and a multi-core processor for optimal performance.\"\\n  }\\n],\\n\"summary\": \"No specific operating system is ideal for the data engineering course, but a 64-bit operating system with at least 8 GB of RAM and a multi-core processor is recommended for optimal performance. For more information, refer to the course materials or contact the course instructor. [Course Materials](https://github.com/DataTalksClub/faq/blob/main/README.md)\"\\n}]></function>'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModelHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m eval_results = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m log_record \u001b[38;5;129;01min\u001b[39;00m tqdm(eval_set):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     eval_result = \u001b[38;5;28;01mawait\u001b[39;00m evaluate_log_record(eval_agent, log_record)\n\u001b[32m      5\u001b[39m     eval_results.append((log_record, eval_result))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mevaluate_log_record\u001b[39m\u001b[34m(eval_agent, log_record)\u001b[39m\n\u001b[32m      9\u001b[39m log = json.dumps(log_simplified)\n\u001b[32m     11\u001b[39m user_prompt = user_prompt_format.format(\n\u001b[32m     12\u001b[39m     instructions=instructions,\n\u001b[32m     13\u001b[39m     question=question,\n\u001b[32m     14\u001b[39m     answer=answer,\n\u001b[32m     15\u001b[39m     log=log\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/agent/abstract.py:218\u001b[39m, in \u001b[36mAbstractAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, deferred_tool_results, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, event_stream_handler)\u001b[39m\n\u001b[32m    204\u001b[39m event_stream_handler = event_stream_handler \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_stream_handler\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    207\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    208\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m     toolsets=toolsets,\n\u001b[32m    217\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    220\u001b[39m             \u001b[38;5;28mself\u001b[39m.is_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_call_tools_node(node)\n\u001b[32m    221\u001b[39m         ):\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m node.stream(agent_run.ctx) \u001b[38;5;28;01mas\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/run.py:149\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    147\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n\u001b[32m    148\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(node=next_node):\n\u001b[32m    151\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_graph/graph.py:758\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_graph/graph.py:731\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    729\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    730\u001b[39m         ctx = GraphRunContext(state=\u001b[38;5;28mself\u001b[39m.state, deps=\u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    734\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:399\u001b[39m, in \u001b[36mModelRequestNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._did_stream:\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# `self._result` gets set when exiting the `stream` contextmanager, so hitting this\u001b[39;00m\n\u001b[32m    396\u001b[39m     \u001b[38;5;66;03m# means that the stream was started but not finished before `run()` was called\u001b[39;00m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.AgentRunError(\u001b[33m'\u001b[39m\u001b[33mYou must finish streaming before calling run()\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_request(ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:441\u001b[39m, in \u001b[36mModelRequestNode._make_request\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    440\u001b[39m model_settings, model_request_parameters, message_history, _ = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prepare_request(ctx)\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m model_response = \u001b[38;5;28;01mawait\u001b[39;00m ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n\u001b[32m    442\u001b[39m ctx.state.usage.requests += \u001b[32m1\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish_handling(ctx, model_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/models/groq.py:186\u001b[39m, in \u001b[36mGroqModel.request\u001b[39m\u001b[34m(self, messages, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    184\u001b[39m check_allow_model_requests()\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._completions_create(\n\u001b[32m    187\u001b[39m         messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(GroqModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[32m    188\u001b[39m     )\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ModelHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.body, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# pragma: no branch\u001b[39;00m\n\u001b[32m    191\u001b[39m         \u001b[38;5;66;03m# The Groq SDK tries to be helpful by raising an exception when generated tool arguments don't match the schema,\u001b[39;00m\n\u001b[32m    192\u001b[39m         \u001b[38;5;66;03m# but we'd rather handle it ourselves so we can tell the model to retry the tool call.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic_ai/models/groq.py:304\u001b[39m, in \u001b[36mGroqModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (status_code := e.status_code) >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ModelHTTPError(status_code=status_code, model_name=\u001b[38;5;28mself\u001b[39m.model_name, body=e.body) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mModelHTTPError\u001b[39m: status_code: 400, model_name: llama-3.3-70b-versatile, body: {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=final_result><![CDATA[{\\n\"checklist\": [\\n  {\\n    \"check_name\": \"Ideal Operating System\",\\n    \"check_pass\": false,\\n    \"justification\": \"No specific information was found regarding the ideal operating system for the data engineering course. However, most data engineering tasks can be performed on any operating system that supports the necessary tools and software, such as Linux, macOS, or Windows.\"\\n  },\\n  {\\n    \"check_name\": \"System Recommendations\",\\n    \"check_pass\": true,\\n    \"justification\": \"It\\'s generally recommended to use a 64-bit operating system with at least 8 GB of RAM and a multi-core processor for optimal performance.\"\\n  }\\n],\\n\"summary\": \"No specific operating system is ideal for the data engineering course, but a 64-bit operating system with at least 8 GB of RAM and a multi-core processor is recommended for optimal performance. For more information, refer to the course materials or contact the course instructor. [Course Materials](https://github.com/DataTalksClub/faq/blob/main/README.md)\"\\n}]></function>'}}"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdeba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a1515d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "545ea2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fcda611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anaconda Version    1.0\n",
       "Pip Version         1.0\n",
       "Package Cache       1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.mean(numeric_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1d34192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(search_function, test_queries):\n",
    "    results = []\n",
    "    \n",
    "    for query, expected_docs in test_queries:\n",
    "        search_results = search_function(query, num_results=5)\n",
    "        \n",
    "        # Calculate hit rate\n",
    "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
    "        \n",
    "        # Calculate MRR\n",
    "        for i, doc in enumerate(search_results):\n",
    "            if doc['filename'] in expected_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        else:\n",
    "            mrr = 0\n",
    "            \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'hit': relevant_found,\n",
    "            'mrr': mrr\n",
    "        })\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fba82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f738ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "7-days-ai-agents-email-crash-course (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
